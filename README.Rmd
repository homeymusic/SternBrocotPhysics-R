---
title: "Stern-Brocot Physics"
output:
  github_document: default
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here())
library(data.table)
library(ggplot2)
```

```{r build-and-load, echo=F, message=F, include=F}
# Rebuild the package and load it into the environment
pkgbuild::compile_dll() 
devtools::load_all(".")

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  warning = FALSE,
  message = FALSE,
  fig.crop = TRUE
)
```

```{r jump-detection, echo=F}
# 1. Load Summary
summary_path <- here::here("data-raw", "outputs", "02_aggregated_summary", "02_aggregated_summary.csv.gz")
dt_summary <- data.table::fread(summary_path)
data.table::setorder(dt_summary, momentum)

# CRITICAL FILTER: Remove unphysical data where P < 1.0
dt_summary <- dt_summary[momentum >= 1.0]

# 2. Sensitivity-based Jump Detection
# Now working only with the physical data range
z_diffs <- abs(diff(dt_summary$zurek_entropy_median))
# Threshold: 5% of max jump to filter out numerical noise
threshold <- max(z_diffs, na.rm = TRUE) * 0.05 
jump_indices <- which(z_diffs > threshold)

# 3. Dynamic Build with Integer Plateau logic
jumps <- lapply(seq_along(jump_indices), function(i) {
  idx <- jump_indices[i]
  m_at <- dt_summary$momentum[idx + 1]
  
  # Logic: Plateau below at floor - 0.1, Plateau above at ceiling + 0.1
  list(
    n = paste("Transition", i),
    p_vals = c(
      (floor(m_at * 10) / 10) - 0.1,   # LOWER PLATEAU (e.g., 13.4 -> 13.3)
      m_at,                           # JUMP POINT    (e.g., 13.42)
      (ceiling(m_at * 10) / 10) + 0.1 # UPPER PLATEAU (e.g., 13.4 -> 13.5)
    )
  )
})

p_markers <- dt_summary$momentum[jump_indices]
```

```{r child = 'inst/readme/introduction.Rmd'}
# Introduction content pulled from external file
```
```{r jump-fluctuations, fig.width=10, fig.height=3 * length(jumps), echo=F}
# 1. Setup Plotting Grid
par(mfrow = c(length(jumps), 3), mar = c(4, 4, 3, 1), oma = c(2, 2, 5, 1))

# 2. Execution Loop
for (j in seq_along(jumps)) {
  for (p in seq_along(jumps[[j]]$p_vals)) {

    current_p <- jumps[[j]]$p_vals[p]
    f_name <- sprintf("micro_macro_erasures_P_%013.6f.csv.gz", round(current_p, 6))
    f_path <- here::here("data-raw", "outputs", "01_micro_macro_erasures", f_name)

    # CRITICAL FILTER: Check file existence AND P >= 1.0 rule
    if (file.exists(f_path) && current_p >= 1.0) {
      raw_fluc <- data.table::fread(f_path, select = "fluctuation")$fluctuation

      # DYNAMIC BIN WIDTH: Determine range per-file, but keep exactly 401 bins (402 breakpoints)
      f_range <- range(raw_fluc, na.rm = TRUE)
      panel_breaks <- seq(f_range[1], f_range[2], length.out = 402)
      
      # Using black as color per your previous instruction
      hist(
        raw_fluc,
        breaks = panel_breaks, # Forced 401 bins, but width is data-driven
        main = bquote(.(jumps[[j]]$n) ~ "at" ~ P == .(round(current_p, 2))),
        xlab = expression(delta[x]),
        ylab = "",
        col = "black",
        las = 1
      )
      
    } else {
      plot.new()
      text(0.5, 0.5, paste("P =", current_p, "\n excluded"), cex = 0.8)
    }
  }
}

# 3. Matrix Annotations
mtext("Automated Detection of Transition Phases (P >= 1.0)", side = 3, line = 2, outer = TRUE, cex = 1.5, font = 2)
mtext("LOWER PLATEAU", side = 3, line = 0, outer = TRUE, adj = 0.15, font = 2, col = "steelblue")
mtext("JUMP POINT", side = 3, line = 0, outer = TRUE, adj = 0.5, font = 2, col = "firebrick")
mtext("UPPER PLATEAU", side = 3, line = 0, outer = TRUE, adj = 0.85, font = 2, col = "steelblue")
```


```{r plot-all-measures, fig.width=6, fig.height=6, echo=F, results='asis'}
# --- Plotting function for aggregated metrics ---
# Data dt_summary is already filtered P >= 1.0

target_cols <- c(
  "kolmogorov_complexity", "shannon_entropy", "zurek_entropy",
  "fluctuation", "numerator", "denominator"
)

render_plot <- function(data, metric_name, vlines) {
  m_mean <- paste0(metric_name, "_mean")
  m_median <- paste0(metric_name, "_median")
  m_sd <- paste0(metric_name, "_sd")
  
  plot_dt <- data[, .(
    momentum = as.numeric(momentum),
    y_mean = as.numeric(get(m_mean)),
    y_med = as.numeric(get(m_median)),
    y_sd = as.numeric(get(m_sd))
  )]

  # Ensure we filter any lingering P<1 values here too, just in case
  plot_dt <- plot_dt[momentum >= 1.0]

  ggplot(plot_dt, aes(x = momentum)) +
    geom_vline(xintercept = vlines[vlines >= 1.0], color = "grey70", linetype = "dashed", alpha = 0.5) +
    geom_ribbon(aes(ymin = y_mean - y_sd, ymax = y_mean + y_sd), fill = "firebrick", alpha = 0.1) +
    geom_line(aes(y = y_mean, color = "Mean"), linewidth = 1) +
    geom_line(aes(y = y_med, color = "Median"), linewidth = 1, linetype = "dashed") +
    scale_color_manual(values = c("Mean" = "firebrick", "Median" = "blue")) +
    labs(title = metric_name, 
         subtitle = "Dashed lines indicate detected Zurek transitions (P >= 1.0)",
         x = "Momentum (P)", y = "Value", color = "Stat") +
    theme_minimal()
}

for (metric in target_cols) {
  if (paste0(metric, "_mean") %in% names(dt_summary)) {
    print(render_plot(dt_summary, metric, p_markers))
  }
}
```

```{r plot-complexity-vs-denominator, fig.width=8, fig.height=6, echo=FALSE}
# Relationship between complexity and rational denominator
# dt_summary is already filtered
ggplot(dt_summary, aes(x = denominator_median, y = kolmogorov_complexity_median, color = momentum)) +
  geom_point(alpha = 0.7, size = 3) +
  scale_color_viridis_c(option = "plasma") +
  labs(
    title = "Scatter Plot: Kolmogorov Complexity vs. Denominator (P >= 1.0)",
    x = "Median Denominator",
    y = "Median Kolmogorov Complexity",
    color = "Momentum (P)"
  ) +
  theme_minimal()
```

```{r plot-all-metrics-with-residuals, fig.width=8, fig.height=8, echo=FALSE, results='asis'}
# dt_summary is already filtered P >= 1.0

# Function to generate the two-panel plot for any given metric
render_dual_plot <- function(metric_name, data_summary, transition_markers) {
  
  # 1. Prepare data for the specific metric
  m_mean   <- paste0(metric_name, "_mean")
  m_median <- paste0(metric_name, "_median")
  m_sd     <- paste0(metric_name, "_sd")

  plot_dt <- data_summary[, .(
    momentum = as.numeric(momentum),
    y_mean   = as.numeric(get(m_mean)),
    y_med    = as.numeric(get(m_median)),
    y_sd     = as.numeric(get(m_sd))
  )]

  # Ensure any P<1 data is removed before sqrt(P) calculation
  plot_dt <- plot_dt[momentum >= 1.0]

  # 2. Calculate 'k' constant and the theory line (y = k * sqrt(P))
  # Remove P=0 which causes issues with sqrt(P) in the denominator
  plot_dt_fit <- plot_dt[momentum > 0.001] 
  k_fit <- mean(plot_dt_fit$y_mean / sqrt(plot_dt_fit$momentum), na.rm = TRUE)
  plot_dt[, theory := k_fit * sqrt(momentum)]
  plot_dt[, residuals := y_mean - theory]

  # 3. Plot 1: Main Data vs Theory
  p_main <- ggplot(plot_dt, aes(x = momentum)) +
    # Only show vlines for physical P >= 1.0
    geom_vline(xintercept = transition_markers[transition_markers >= 1.0], color = "grey70", linetype = "dashed", alpha = 0.5) +
    geom_ribbon(aes(ymin = y_mean - y_sd, ymax = y_mean + y_sd), fill = "firebrick", alpha = 0.1) +
    geom_line(aes(y = y_mean, color = "Mean"), linewidth = 1) +
    geom_line(aes(y = y_med, color = "Median"), linewidth = 1, linetype = "dashed") +
    geom_line(aes(y = theory, color = "Theory (sqrt P)"), linewidth = 0.8, alpha = 0.8) +
    scale_color_manual(values = c("Mean" = "firebrick", "Median" = "blue", "Theory (sqrt P)" = "forestgreen")) +
    labs(
      title = paste(tools::toTitleCase(metric_name), "vs. Momentum (P >= 1.0)"),
      subtitle = bquote("Holographic Fit:" ~ y %prop% sqrt(P) ~ (k %approx% .(round(k_fit, 3)))),
      x = "Momentum (P)", y = "Value", color = "Series"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom", plot.title = element_text(face = "bold"),
          axis.text.x = element_blank(), axis.title.x = element_blank())

  # 4. Plot 2: Residuals Plot (Mean - Theory)
  p_residuals <- ggplot(plot_dt, aes(x = momentum, y = residuals)) +
    geom_bar(stat = "identity", fill = "gray50", alpha = 0.6) +
    geom_hline(yintercept = 0, color = "red", linetype = "solid", linewidth = 1) +
    labs(y = "Residual", x = "Momentum (P)") +
    theme_minimal() +
    theme(plot.title = element_blank())

  # 5. Combine and print
  if(requireNamespace("cowplot", quietly = TRUE)) {
    print(cowplot::plot_grid(p_main, p_residuals, ncol = 1, align = "v", rel_heights = c(3, 1)))
  } else {
    print(p_main)
    print(p_residuals)
  }
}

# --- Execute the loop for the three main metrics ---
target_metrics <- c("shannon_entropy", "zurek_entropy", "kolmogorov_complexity")

for (metric in target_metrics) {
  if (paste0(metric, "_mean") %in% names(dt_summary)) {
    render_dual_plot(metric, dt_summary, p_markers)
  }
}
```
### Fit Statistics: Holographic Theory ($y \propto \sqrt{P}$)

This section summarizes the goodness-of-fit for the mean values of Shannon, Kolmogorov, and Zurek entropies against the proposed holographic scaling relation for physical momentum values ($P \ge 1.0$).

```{r fit-statistics, echo=FALSE, results='asis'}
# Use the dt_summary data.table object available from the 'jump-detection' chunk

calculate_fit_stats <- function(metric_name, data_summary) {
  mean_col <- paste0(metric_name, "_mean")
  
  if (!(mean_col %in% names(data_summary))) {
    return(NULL)
  }
  
  plot_dt <- data_summary[, .(
    momentum = as.numeric(momentum),
    y_mean   = as.numeric(get(mean_col))
  )]

  # CRITICAL FILTER: Ensure all data used for the fit is P >= 1.0 (physical range)
  # Also remove P=0 which causes issues with sqrt(P) in the denominator
  plot_dt <- plot_dt[momentum >= 1.0]
  
  if (nrow(plot_dt) == 0) return(NULL) # Return if no physical data remains
  
  # Calculate 'k' constant and the theory line
  k_fit <- mean(plot_dt$y_mean / sqrt(plot_dt$momentum), na.rm = TRUE)
  plot_dt[, theory := k_fit * sqrt(momentum)]

  # Calculate R-squared (using sum of squares approach)
  ss_total <- sum((plot_dt$y_mean - mean(plot_dt$y_mean, na.rm = TRUE))^2, na.rm = TRUE)
  ss_residual <- sum((plot_dt$y_mean - plot_dt$theory)^2, na.rm = TRUE)
  r_squared <- 1 - (ss_residual / ss_total)
  
  # Calculate RMSE
  rmse <- sqrt(mean((plot_dt$y_mean - plot_dt$theory)^2, na.rm = TRUE))

  # Calculate Pearson correlation (R value)
  correlation_r <- cor(plot_dt$y_mean, plot_dt$theory, use = "complete.obs")
  
  return(list(
    Metric = tools::toTitleCase(gsub("_", " ", metric_name)),
    k_constant = round(k_fit, 3),
    R_squared = round(r_squared, 4),
    Correlation_R = round(correlation_r, 4),
    RMSE = round(rmse, 4)
  ))
}

target_metrics <- c("shannon_entropy", "zurek_entropy", "kolmogorov_complexity")
stats_list <- lapply(target_metrics, calculate_fit_stats, data_summary = dt_summary)
stats_df <- as.data.frame(do.call(rbind, stats_list))

# Render as Markdown table
knitr::kable(stats_df)
```

### Summary and Conclusion

The quantitative analysis of the physical data ($P \ge 1.0$) provides strong evidence that the **Zurek entropy** best aligns with the hypothesized **holographic principle** scaling.

The Zurek and Kolmogorov complexity metrics both exhibit outstanding fits to the $y \propto \sqrt{P}$ theory curve, with R-squared ($R^2$) values of **0.9827** and **0.9820**, respectively. This suggests that the algorithmic information and erasure metrics strongly adhere to this theoretical scaling behavior within the physical regime.

In contrast, the Shannon entropy metric yields a highly negative $R^2$ value of **-2.803**, indicating a poor fit to the same model. This confirms the visual observation that the Shannon component is likely the source of the structured, "quantized" deviations from the continuous holographic theory curve.
